{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2513ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aamena/Assessment_tcs/.venv/lib/python3.10/site-packages/detectron2/model_zoo/model_zoo.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/19 17:54:15 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /home/aamena/Assessment_tcs/output/domain_a_baseline/model_final.pth ...\n",
      "\u001b[32m[11/19 17:54:15 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/19 17:54:15 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[11/19 17:54:15 d2.data.datasets.coco]: \u001b[0mLoaded 1728 images in COCO format from datasets/NEU-seg-DOMAIN_A2/annotations.coco.json\n",
      "\u001b[32m[11/19 17:54:15 d2.data.build]: \u001b[0mDistribution of instances among all 4 categories:\n",
      "\u001b[36m|   category    | #instances   |  category  | #instances   |  category  | #instances   |\n",
      "|:-------------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
      "| In-\"Pa\",-\"Sc\" | 0            |     In     | 1705         |     Pa     | 1240         |\n",
      "|      Sc       | 1162         |            |              |            |              |\n",
      "|     total     | 4107         |            |              |            |              |\u001b[0m\n",
      "\u001b[32m[11/19 17:54:15 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[11/19 17:54:15 d2.data.common]: \u001b[0mSerializing 1728 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/19 17:54:15 d2.data.common]: \u001b[0mSerialized dataset takes 1.70 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/19 17:54:15 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "ðŸš€ Starting evaluation...\n",
      "\u001b[32m[11/19 17:54:15 d2.evaluation.evaluator]: \u001b[0mStart inference on 1728 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aamena/Assessment_tcs/.venv/lib/python3.10/site-packages/torch/functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "W1119 17:54:15.744000 75665 torch/fx/_symbolic_trace.py:52] is_fx_tracing will return true for both fx.symbolic_trace and torch.export. Please use is_fx_tracing_symbolic_tracing() for specifically fx.symbolic_trace or torch.compiler.is_compiling() for specifically torch.export/compile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/19 17:54:16 d2.evaluation.evaluator]: \u001b[0mInference done 11/1728. Dataloading: 0.0006 s/iter. Inference: 0.0592 s/iter. Eval: 0.0014 s/iter. Total: 0.0612 s/iter. ETA=0:01:45\n",
      "\u001b[32m[11/19 17:54:21 d2.evaluation.evaluator]: \u001b[0mInference done 92/1728. Dataloading: 0.0008 s/iter. Inference: 0.0597 s/iter. Eval: 0.0016 s/iter. Total: 0.0621 s/iter. ETA=0:01:41\n",
      "\u001b[32m[11/19 17:54:26 d2.evaluation.evaluator]: \u001b[0mInference done 173/1728. Dataloading: 0.0009 s/iter. Inference: 0.0598 s/iter. Eval: 0.0015 s/iter. Total: 0.0621 s/iter. ETA=0:01:36\n",
      "\u001b[32m[11/19 17:54:31 d2.evaluation.evaluator]: \u001b[0mInference done 254/1728. Dataloading: 0.0009 s/iter. Inference: 0.0598 s/iter. Eval: 0.0015 s/iter. Total: 0.0622 s/iter. ETA=0:01:31\n",
      "\u001b[32m[11/19 17:54:36 d2.evaluation.evaluator]: \u001b[0mInference done 335/1728. Dataloading: 0.0009 s/iter. Inference: 0.0598 s/iter. Eval: 0.0015 s/iter. Total: 0.0622 s/iter. ETA=0:01:26\n",
      "\u001b[32m[11/19 17:54:41 d2.evaluation.evaluator]: \u001b[0mInference done 416/1728. Dataloading: 0.0009 s/iter. Inference: 0.0599 s/iter. Eval: 0.0015 s/iter. Total: 0.0623 s/iter. ETA=0:01:21\n",
      "\u001b[32m[11/19 17:54:46 d2.evaluation.evaluator]: \u001b[0mInference done 496/1728. Dataloading: 0.0009 s/iter. Inference: 0.0600 s/iter. Eval: 0.0015 s/iter. Total: 0.0624 s/iter. ETA=0:01:16\n",
      "\u001b[32m[11/19 17:54:51 d2.evaluation.evaluator]: \u001b[0mInference done 576/1728. Dataloading: 0.0009 s/iter. Inference: 0.0601 s/iter. Eval: 0.0015 s/iter. Total: 0.0624 s/iter. ETA=0:01:11\n",
      "\u001b[32m[11/19 17:54:56 d2.evaluation.evaluator]: \u001b[0mInference done 656/1728. Dataloading: 0.0009 s/iter. Inference: 0.0602 s/iter. Eval: 0.0015 s/iter. Total: 0.0625 s/iter. ETA=0:01:07\n",
      "\u001b[32m[11/19 17:55:01 d2.evaluation.evaluator]: \u001b[0mInference done 736/1728. Dataloading: 0.0009 s/iter. Inference: 0.0602 s/iter. Eval: 0.0015 s/iter. Total: 0.0626 s/iter. ETA=0:01:02\n",
      "\u001b[32m[11/19 17:55:06 d2.evaluation.evaluator]: \u001b[0mInference done 816/1728. Dataloading: 0.0009 s/iter. Inference: 0.0602 s/iter. Eval: 0.0015 s/iter. Total: 0.0626 s/iter. ETA=0:00:57\n",
      "\u001b[32m[11/19 17:55:11 d2.evaluation.evaluator]: \u001b[0mInference done 896/1728. Dataloading: 0.0009 s/iter. Inference: 0.0603 s/iter. Eval: 0.0015 s/iter. Total: 0.0627 s/iter. ETA=0:00:52\n",
      "\u001b[32m[11/19 17:55:17 d2.evaluation.evaluator]: \u001b[0mInference done 976/1728. Dataloading: 0.0009 s/iter. Inference: 0.0603 s/iter. Eval: 0.0015 s/iter. Total: 0.0627 s/iter. ETA=0:00:47\n",
      "\u001b[32m[11/19 17:55:22 d2.evaluation.evaluator]: \u001b[0mInference done 1056/1728. Dataloading: 0.0009 s/iter. Inference: 0.0603 s/iter. Eval: 0.0015 s/iter. Total: 0.0627 s/iter. ETA=0:00:42\n",
      "\u001b[32m[11/19 17:55:27 d2.evaluation.evaluator]: \u001b[0mInference done 1136/1728. Dataloading: 0.0009 s/iter. Inference: 0.0603 s/iter. Eval: 0.0015 s/iter. Total: 0.0627 s/iter. ETA=0:00:37\n",
      "\u001b[32m[11/19 17:55:32 d2.evaluation.evaluator]: \u001b[0mInference done 1216/1728. Dataloading: 0.0009 s/iter. Inference: 0.0604 s/iter. Eval: 0.0015 s/iter. Total: 0.0627 s/iter. ETA=0:00:32\n",
      "\u001b[32m[11/19 17:55:37 d2.evaluation.evaluator]: \u001b[0mInference done 1296/1728. Dataloading: 0.0009 s/iter. Inference: 0.0604 s/iter. Eval: 0.0014 s/iter. Total: 0.0627 s/iter. ETA=0:00:27\n",
      "\u001b[32m[11/19 17:55:42 d2.evaluation.evaluator]: \u001b[0mInference done 1376/1728. Dataloading: 0.0009 s/iter. Inference: 0.0604 s/iter. Eval: 0.0015 s/iter. Total: 0.0628 s/iter. ETA=0:00:22\n",
      "\u001b[32m[11/19 17:55:47 d2.evaluation.evaluator]: \u001b[0mInference done 1456/1728. Dataloading: 0.0009 s/iter. Inference: 0.0604 s/iter. Eval: 0.0015 s/iter. Total: 0.0628 s/iter. ETA=0:00:17\n",
      "\u001b[32m[11/19 17:55:52 d2.evaluation.evaluator]: \u001b[0mInference done 1536/1728. Dataloading: 0.0009 s/iter. Inference: 0.0604 s/iter. Eval: 0.0015 s/iter. Total: 0.0628 s/iter. ETA=0:00:12\n",
      "\u001b[32m[11/19 17:55:57 d2.evaluation.evaluator]: \u001b[0mInference done 1615/1728. Dataloading: 0.0009 s/iter. Inference: 0.0604 s/iter. Eval: 0.0015 s/iter. Total: 0.0628 s/iter. ETA=0:00:07\n",
      "\u001b[32m[11/19 17:56:02 d2.evaluation.evaluator]: \u001b[0mInference done 1694/1728. Dataloading: 0.0009 s/iter. Inference: 0.0605 s/iter. Eval: 0.0015 s/iter. Total: 0.0629 s/iter. ETA=0:00:02\n",
      "\u001b[32m[11/19 17:56:04 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:48.328659 (0.062872 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/19 17:56:04 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:44 (0.060461 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/19 17:56:04 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[11/19 17:56:04 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to /home/aamena/Assessment_tcs/output/domain_a_baseline/validation_results/coco_instances_results.json\n",
      "\u001b[32m[11/19 17:56:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[11/19 17:56:04 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[11/19 17:56:04 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.15 seconds.\n",
      "\u001b[32m[11/19 17:56:04 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[11/19 17:56:04 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.603\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.918\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.666\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.182\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.465\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.685\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.320\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.688\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.688\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.341\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.586\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.762\n",
      "\u001b[32m[11/19 17:56:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 60.292 | 91.753 | 66.566 | 18.230 | 46.454 | 68.497 |\n",
      "\u001b[32m[11/19 17:56:04 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category      | AP     | category   | AP     | category   | AP     |\n",
      "|:--------------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| In-\"Pa\",-\"Sc\" | nan    | In         | 53.122 | Pa         | 74.302 |\n",
      "| Sc            | 53.452 |            |        |            |        |\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[11/19 17:56:04 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
      "\u001b[32m[11/19 17:56:04 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.17 seconds.\n",
      "\u001b[32m[11/19 17:56:04 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[11/19 17:56:04 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.02 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.488\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.865\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.501\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.075\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.560\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.270\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.579\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.579\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.276\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.498\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.627\n",
      "\u001b[32m[11/19 17:56:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 48.787 | 86.492 | 50.122 | 7.518 | 34.629 | 55.979 |\n",
      "\u001b[32m[11/19 17:56:04 d2.evaluation.coco_evaluation]: \u001b[0mPer-category segm AP: \n",
      "| category      | AP     | category   | AP     | category   | AP     |\n",
      "|:--------------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| In-\"Pa\",-\"Sc\" | nan    | In         | 43.657 | Pa         | 67.340 |\n",
      "| Sc            | 35.364 |            |        |            |        |\n",
      "\n",
      "================================\n",
      " FINAL EVALUATION RESULTS\n",
      "================================\n",
      "OrderedDict([('bbox', {'AP': 60.29203042732125, 'AP50': 91.75307118487301, 'AP75': 66.5658298183014, 'APs': 18.229974571223586, 'APm': 46.45434255683282, 'APl': 68.49659358327706, 'AP-In-\"Pa\",-\"Sc\"': nan, 'AP-In': 53.122025018894504, 'AP-Pa': 74.30218076852493, 'AP-Sc': 53.451885494544314}), ('segm', {'AP': 48.78672317355928, 'AP50': 86.49157792207677, 'AP75': 50.1223220790401, 'APs': 7.518243479013104, 'APm': 34.62862060319074, 'APl': 55.978871810340195, 'AP-In-\"Pa\",-\"Sc\"': nan, 'AP-In': 43.65677206708757, 'AP-Pa': 67.33975997168785, 'AP-Sc': 35.36363748190242})])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import detectron2\n",
    "\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data import DatasetMapper\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1. Register Dataset\n",
    "# ----------------------------------------------------------\n",
    "VAL_DATASET_NAME = \"neu_valid\"\n",
    "VAL_JSON = \"<Domain_A_evaluation_data_annotations_json_file>\"\n",
    "VAL_IMG_DIR = \"<Domain_A_evaluation_data_images>\"\n",
    "\n",
    "# Register once\n",
    "try:\n",
    "    register_coco_instances(VAL_DATASET_NAME, {}, VAL_JSON, VAL_IMG_DIR)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2. Configuration\n",
    "# ----------------------------------------------------------\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\n",
    "    \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    "))\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "\n",
    "cfg.OUTPUT_DIR = \"/home/aamena/Assessment_tcs/output/domain_a_baseline\"\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3. Evaluator + Loader\n",
    "# ----------------------------------------------------------\n",
    "predictor = DefaultPredictor(cfg)\n",
    "test_mapper = DatasetMapper(cfg, is_train=False)\n",
    "\n",
    "# FIXED: correct arguments\n",
    "eval_dataloader = build_detection_test_loader(\n",
    "    cfg,\n",
    "    VAL_DATASET_NAME,\n",
    "    mapper=test_mapper\n",
    ")\n",
    "\n",
    "evaluator = COCOEvaluator(\n",
    "    VAL_DATASET_NAME,\n",
    "    cfg,\n",
    "    False,\n",
    "    output_dir=os.path.join(cfg.OUTPUT_DIR, \"validation_results\")\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4. Run Evaluation\n",
    "# ----------------------------------------------------------\n",
    "print(\"ðŸš€ Starting evaluation...\")\n",
    "\n",
    "results = inference_on_dataset(\n",
    "    predictor.model,\n",
    "    eval_dataloader,\n",
    "    evaluator\n",
    ")\n",
    "\n",
    "print(\"\\n================================\")\n",
    "print(\" FINAL EVALUATION RESULTS\")\n",
    "print(\"================================\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c6d958e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Bounding Box Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AP</th>\n",
       "      <td>60.292030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP50</th>\n",
       "      <td>91.753071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP75</th>\n",
       "      <td>66.565830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APs</th>\n",
       "      <td>18.229975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APm</th>\n",
       "      <td>46.454343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APl</th>\n",
       "      <td>68.496594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-In-\"Pa\",-\"Sc\"</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-In</th>\n",
       "      <td>53.122025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-Pa</th>\n",
       "      <td>74.302181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-Sc</th>\n",
       "      <td>53.451885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Score\n",
       "AP                60.292030\n",
       "AP50              91.753071\n",
       "AP75              66.565830\n",
       "APs               18.229975\n",
       "APm               46.454343\n",
       "APl               68.496594\n",
       "AP-In-\"Pa\",-\"Sc\"        NaN\n",
       "AP-In             53.122025\n",
       "AP-Pa             74.302181\n",
       "AP-Sc             53.451885"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Œ Segmentation Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AP</th>\n",
       "      <td>48.786723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP50</th>\n",
       "      <td>86.491578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP75</th>\n",
       "      <td>50.122322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APs</th>\n",
       "      <td>7.518243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APm</th>\n",
       "      <td>34.628621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APl</th>\n",
       "      <td>55.978872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-In-\"Pa\",-\"Sc\"</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-In</th>\n",
       "      <td>43.656772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-Pa</th>\n",
       "      <td>67.339760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-Sc</th>\n",
       "      <td>35.363637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Score\n",
       "AP                48.786723\n",
       "AP50              86.491578\n",
       "AP75              50.122322\n",
       "APs                7.518243\n",
       "APm               34.628621\n",
       "APl               55.978872\n",
       "AP-In-\"Pa\",-\"Sc\"        NaN\n",
       "AP-In             43.656772\n",
       "AP-Pa             67.339760\n",
       "AP-Sc             35.363637"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------\n",
    "# Convert Detectron2 COCO results â†’ DataFrames\n",
    "# ------------------------------------------\n",
    "bbox_results = results[\"bbox\"]\n",
    "segm_results = results[\"segm\"]\n",
    "\n",
    "bbox_df = pd.DataFrame(bbox_results, index=[0]).T.rename(columns={0: \"Score\"})\n",
    "segm_df = pd.DataFrame(segm_results, index=[0]).T.rename(columns={0: \"Score\"})\n",
    "\n",
    "print(\"ðŸ“Œ Bounding Box Metrics\")\n",
    "display(bbox_df)\n",
    "\n",
    "print(\"\\nðŸ“Œ Segmentation Metrics\")\n",
    "display(segm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ebb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aamena/Assessment_tcs/.venv/lib/python3.10/site-packages/detectron2/model_zoo/model_zoo.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/19 16:32:07 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /home/aamena/Assessment_tcs/output/domain_a_baseline/model_final.pth ...\n",
      "\u001b[32m[11/19 16:32:07 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/19 16:32:07 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[11/19 16:32:07 d2.data.datasets.coco]: \u001b[0mLoaded 1728 images in COCO format from datasets/NEU-seg-DOMAIN_B2OR/annotations.coco.json\n",
      "\u001b[32m[11/19 16:32:07 d2.data.build]: \u001b[0mDistribution of instances among all 4 categories:\n",
      "\u001b[36m|   category    | #instances   |  category  | #instances   |  category  | #instances   |\n",
      "|:-------------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
      "| In-\"Pa\",-\"Sc\" | 0            |     In     | 1705         |     Pa     | 1240         |\n",
      "|      Sc       | 1162         |            |              |            |              |\n",
      "|     total     | 4107         |            |              |            |              |\u001b[0m\n",
      "\u001b[32m[11/19 16:32:07 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[11/19 16:32:07 d2.data.common]: \u001b[0mSerializing 1728 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/19 16:32:07 d2.data.common]: \u001b[0mSerialized dataset takes 1.71 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/19 16:32:07 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "ðŸš€ Starting evaluation...\n",
      "\u001b[32m[11/19 16:32:07 d2.evaluation.evaluator]: \u001b[0mStart inference on 1728 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aamena/Assessment_tcs/.venv/lib/python3.10/site-packages/torch/functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "W1119 16:32:08.242000 52009 torch/fx/_symbolic_trace.py:52] is_fx_tracing will return true for both fx.symbolic_trace and torch.export. Please use is_fx_tracing_symbolic_tracing() for specifically fx.symbolic_trace or torch.compiler.is_compiling() for specifically torch.export/compile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/19 16:32:09 d2.evaluation.evaluator]: \u001b[0mInference done 11/1728. Dataloading: 0.0006 s/iter. Inference: 0.0605 s/iter. Eval: 0.0010 s/iter. Total: 0.0621 s/iter. ETA=0:01:46\n",
      "\u001b[32m[11/19 16:32:14 d2.evaluation.evaluator]: \u001b[0mInference done 92/1728. Dataloading: 0.0008 s/iter. Inference: 0.0605 s/iter. Eval: 0.0011 s/iter. Total: 0.0624 s/iter. ETA=0:01:42\n",
      "\u001b[32m[11/19 16:32:19 d2.evaluation.evaluator]: \u001b[0mInference done 172/1728. Dataloading: 0.0008 s/iter. Inference: 0.0605 s/iter. Eval: 0.0011 s/iter. Total: 0.0625 s/iter. ETA=0:01:37\n",
      "\u001b[32m[11/19 16:32:24 d2.evaluation.evaluator]: \u001b[0mInference done 252/1728. Dataloading: 0.0009 s/iter. Inference: 0.0606 s/iter. Eval: 0.0011 s/iter. Total: 0.0626 s/iter. ETA=0:01:32\n",
      "\u001b[32m[11/19 16:32:29 d2.evaluation.evaluator]: \u001b[0mInference done 332/1728. Dataloading: 0.0009 s/iter. Inference: 0.0606 s/iter. Eval: 0.0011 s/iter. Total: 0.0626 s/iter. ETA=0:01:27\n",
      "\u001b[32m[11/19 16:32:34 d2.evaluation.evaluator]: \u001b[0mInference done 412/1728. Dataloading: 0.0009 s/iter. Inference: 0.0607 s/iter. Eval: 0.0011 s/iter. Total: 0.0627 s/iter. ETA=0:01:22\n",
      "\u001b[32m[11/19 16:32:39 d2.evaluation.evaluator]: \u001b[0mInference done 492/1728. Dataloading: 0.0009 s/iter. Inference: 0.0607 s/iter. Eval: 0.0011 s/iter. Total: 0.0627 s/iter. ETA=0:01:17\n",
      "\u001b[32m[11/19 16:32:44 d2.evaluation.evaluator]: \u001b[0mInference done 572/1728. Dataloading: 0.0009 s/iter. Inference: 0.0608 s/iter. Eval: 0.0011 s/iter. Total: 0.0627 s/iter. ETA=0:01:12\n",
      "\u001b[32m[11/19 16:32:49 d2.evaluation.evaluator]: \u001b[0mInference done 652/1728. Dataloading: 0.0009 s/iter. Inference: 0.0608 s/iter. Eval: 0.0011 s/iter. Total: 0.0628 s/iter. ETA=0:01:07\n",
      "\u001b[32m[11/19 16:32:54 d2.evaluation.evaluator]: \u001b[0mInference done 731/1728. Dataloading: 0.0009 s/iter. Inference: 0.0609 s/iter. Eval: 0.0011 s/iter. Total: 0.0628 s/iter. ETA=0:01:02\n",
      "\u001b[32m[11/19 16:32:59 d2.evaluation.evaluator]: \u001b[0mInference done 810/1728. Dataloading: 0.0009 s/iter. Inference: 0.0609 s/iter. Eval: 0.0011 s/iter. Total: 0.0629 s/iter. ETA=0:00:57\n",
      "\u001b[32m[11/19 16:33:04 d2.evaluation.evaluator]: \u001b[0mInference done 890/1728. Dataloading: 0.0009 s/iter. Inference: 0.0609 s/iter. Eval: 0.0011 s/iter. Total: 0.0629 s/iter. ETA=0:00:52\n",
      "\u001b[32m[11/19 16:33:09 d2.evaluation.evaluator]: \u001b[0mInference done 970/1728. Dataloading: 0.0009 s/iter. Inference: 0.0609 s/iter. Eval: 0.0011 s/iter. Total: 0.0629 s/iter. ETA=0:00:47\n",
      "\u001b[32m[11/19 16:33:14 d2.evaluation.evaluator]: \u001b[0mInference done 1049/1728. Dataloading: 0.0009 s/iter. Inference: 0.0610 s/iter. Eval: 0.0011 s/iter. Total: 0.0630 s/iter. ETA=0:00:42\n",
      "\u001b[32m[11/19 16:33:19 d2.evaluation.evaluator]: \u001b[0mInference done 1128/1728. Dataloading: 0.0009 s/iter. Inference: 0.0610 s/iter. Eval: 0.0011 s/iter. Total: 0.0630 s/iter. ETA=0:00:37\n",
      "\u001b[32m[11/19 16:33:24 d2.evaluation.evaluator]: \u001b[0mInference done 1207/1728. Dataloading: 0.0009 s/iter. Inference: 0.0611 s/iter. Eval: 0.0011 s/iter. Total: 0.0631 s/iter. ETA=0:00:32\n",
      "\u001b[32m[11/19 16:33:29 d2.evaluation.evaluator]: \u001b[0mInference done 1286/1728. Dataloading: 0.0009 s/iter. Inference: 0.0611 s/iter. Eval: 0.0011 s/iter. Total: 0.0631 s/iter. ETA=0:00:27\n",
      "\u001b[32m[11/19 16:33:34 d2.evaluation.evaluator]: \u001b[0mInference done 1364/1728. Dataloading: 0.0009 s/iter. Inference: 0.0612 s/iter. Eval: 0.0011 s/iter. Total: 0.0632 s/iter. ETA=0:00:23\n",
      "\u001b[32m[11/19 16:33:39 d2.evaluation.evaluator]: \u001b[0mInference done 1443/1728. Dataloading: 0.0009 s/iter. Inference: 0.0613 s/iter. Eval: 0.0011 s/iter. Total: 0.0633 s/iter. ETA=0:00:18\n",
      "\u001b[32m[11/19 16:33:44 d2.evaluation.evaluator]: \u001b[0mInference done 1521/1728. Dataloading: 0.0009 s/iter. Inference: 0.0613 s/iter. Eval: 0.0011 s/iter. Total: 0.0633 s/iter. ETA=0:00:13\n",
      "\u001b[32m[11/19 16:33:49 d2.evaluation.evaluator]: \u001b[0mInference done 1600/1728. Dataloading: 0.0009 s/iter. Inference: 0.0613 s/iter. Eval: 0.0011 s/iter. Total: 0.0633 s/iter. ETA=0:00:08\n",
      "\u001b[32m[11/19 16:33:54 d2.evaluation.evaluator]: \u001b[0mInference done 1679/1728. Dataloading: 0.0009 s/iter. Inference: 0.0614 s/iter. Eval: 0.0011 s/iter. Total: 0.0634 s/iter. ETA=0:00:03\n",
      "\u001b[32m[11/19 16:33:57 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:49.240595 (0.063401 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/19 16:33:57 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:45 (0.061385 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/19 16:33:57 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[11/19 16:33:57 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to /home/aamena/Assessment_tcs/output/domain_a_baseline/validation_results/coco_instances_results.json\n",
      "\u001b[32m[11/19 16:33:57 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[11/19 16:33:57 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[11/19 16:33:58 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.16 seconds.\n",
      "\u001b[32m[11/19 16:33:58 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[11/19 16:33:58 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.422\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.705\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.433\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.291\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.491\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.254\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.497\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.497\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.388\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.564\n",
      "\u001b[32m[11/19 16:33:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 42.169 | 70.484 | 43.284 | 3.212 | 29.102 | 49.063 |\n",
      "\u001b[32m[11/19 16:33:58 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category      | AP     | category   | AP     | category   | AP     |\n",
      "|:--------------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| In-\"Pa\",-\"Sc\" | nan    | In         | 25.067 | Pa         | 61.328 |\n",
      "| Sc            | 40.113 |            |        |            |        |\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[11/19 16:33:58 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
      "\u001b[32m[11/19 16:33:58 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.16 seconds.\n",
      "\u001b[32m[11/19 16:33:58 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[11/19 16:33:58 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.02 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.345\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.660\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.325\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.020\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.213\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.407\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.217\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.424\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.424\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.050\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.327\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.473\n",
      "\u001b[32m[11/19 16:33:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 34.470 | 65.993 | 32.468 | 1.977 | 21.268 | 40.741 |\n",
      "\u001b[32m[11/19 16:33:58 d2.evaluation.coco_evaluation]: \u001b[0mPer-category segm AP: \n",
      "| category      | AP     | category   | AP     | category   | AP     |\n",
      "|:--------------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| In-\"Pa\",-\"Sc\" | nan    | In         | 20.807 | Pa         | 57.285 |\n",
      "| Sc            | 25.317 |            |        |            |        |\n",
      "\n",
      "================================\n",
      " FINAL EVALUATION RESULTS\n",
      "================================\n",
      "OrderedDict([('bbox', {'AP': 42.16934024276164, 'AP50': 70.48448464325486, 'AP75': 43.28419656146809, 'APs': 3.2123527902831266, 'APm': 29.101541491893602, 'APl': 49.062671984243956, 'AP-In-\"Pa\",-\"Sc\"': nan, 'AP-In': 25.06711441036923, 'AP-Pa': 61.32761717547594, 'AP-Sc': 40.113289142439754}), ('segm', {'AP': 34.46990368698417, 'AP50': 65.99304763631389, 'AP75': 32.46785550764259, 'APs': 1.9768248651680782, 'APm': 21.26755532773084, 'APl': 40.74090332665283, 'AP-In-\"Pa\",-\"Sc\"': nan, 'AP-In': 20.807487688192687, 'AP-Pa': 57.28494360347679, 'AP-Sc': 25.31727976928303})])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import detectron2\n",
    "\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data import DatasetMapper\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1. Register Dataset\n",
    "# ----------------------------------------------------------\n",
    "VAL_DATASET_NAME = \"neu_valid\"\n",
    "VAL_JSON = \"datasets/NEU-seg-DOMAIN_B2OR/annotations.coco.json\"\n",
    "VAL_IMG_DIR = \"<Domain_B_Original_evaluation_data_images>\"\n",
    "\n",
    "# Register once\n",
    "try:\n",
    "    register_coco_instances(VAL_DATASET_NAME, {}, VAL_JSON, VAL_IMG_DIR)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2. Configuration\n",
    "# ----------------------------------------------------------\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\n",
    "    \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    "))\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "\n",
    "cfg.OUTPUT_DIR = \"/home/aamena/Assessment_tcs/output/domain_a_baseline\"\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3. Evaluator + Loader\n",
    "# ----------------------------------------------------------\n",
    "predictor = DefaultPredictor(cfg)\n",
    "test_mapper = DatasetMapper(cfg, is_train=False)\n",
    "\n",
    "# FIXED: correct arguments\n",
    "eval_dataloader = build_detection_test_loader(\n",
    "    cfg,\n",
    "    VAL_DATASET_NAME,\n",
    "    mapper=test_mapper\n",
    ")\n",
    "\n",
    "evaluator = COCOEvaluator(\n",
    "    VAL_DATASET_NAME,\n",
    "    cfg,\n",
    "    False,\n",
    "    output_dir=os.path.join(cfg.OUTPUT_DIR, \"validation_results\")\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4. Run Evaluation\n",
    "# ----------------------------------------------------------\n",
    "print(\"ðŸš€ Starting evaluation...\")\n",
    "\n",
    "results = inference_on_dataset(\n",
    "    predictor.model,\n",
    "    eval_dataloader,\n",
    "    evaluator\n",
    ")\n",
    "\n",
    "print(\"\\n================================\")\n",
    "print(\" FINAL EVALUATION RESULTS\")\n",
    "print(\"================================\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cdd7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Bounding Box Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AP</th>\n",
       "      <td>42.169340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP50</th>\n",
       "      <td>70.484485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP75</th>\n",
       "      <td>43.284197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APs</th>\n",
       "      <td>3.212353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APm</th>\n",
       "      <td>29.101541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APl</th>\n",
       "      <td>49.062672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-In-\"Pa\",-\"Sc\"</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-In</th>\n",
       "      <td>25.067114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-Pa</th>\n",
       "      <td>61.327617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-Sc</th>\n",
       "      <td>40.113289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Score\n",
       "AP                42.169340\n",
       "AP50              70.484485\n",
       "AP75              43.284197\n",
       "APs                3.212353\n",
       "APm               29.101541\n",
       "APl               49.062672\n",
       "AP-In-\"Pa\",-\"Sc\"        NaN\n",
       "AP-In             25.067114\n",
       "AP-Pa             61.327617\n",
       "AP-Sc             40.113289"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Œ Segmentation Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AP</th>\n",
       "      <td>34.469904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP50</th>\n",
       "      <td>65.993048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP75</th>\n",
       "      <td>32.467856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APs</th>\n",
       "      <td>1.976825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APm</th>\n",
       "      <td>21.267555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APl</th>\n",
       "      <td>40.740903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-In-\"Pa\",-\"Sc\"</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-In</th>\n",
       "      <td>20.807488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-Pa</th>\n",
       "      <td>57.284944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-Sc</th>\n",
       "      <td>25.317280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Score\n",
       "AP                34.469904\n",
       "AP50              65.993048\n",
       "AP75              32.467856\n",
       "APs                1.976825\n",
       "APm               21.267555\n",
       "APl               40.740903\n",
       "AP-In-\"Pa\",-\"Sc\"        NaN\n",
       "AP-In             20.807488\n",
       "AP-Pa             57.284944\n",
       "AP-Sc             25.317280"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------\n",
    "# Convert Detectron2 COCO results â†’ DataFrames\n",
    "# ------------------------------------------\n",
    "bbox_results = results[\"bbox\"]\n",
    "segm_results = results[\"segm\"]\n",
    "\n",
    "bbox_df = pd.DataFrame(bbox_results, index=[0]).T.rename(columns={0: \"Score\"})\n",
    "segm_df = pd.DataFrame(segm_results, index=[0]).T.rename(columns={0: \"Score\"})\n",
    "\n",
    "print(\"ðŸ“Œ Bounding Box Metrics\")\n",
    "display(bbox_df)\n",
    "\n",
    "print(\"\\nðŸ“Œ Segmentation Metrics\")\n",
    "display(segm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a501dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 1728 image entries.\n",
      "Saved: /home/aamena/Assessment_tcs/datasets/DomainB_Enhanced/annotations_fixed.coco.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "images_dir = \"/home/aamena/Assessment_tcs/datasets/DomainB_Enhanced/images\"\n",
    "ann_path = \"/home/aamena/Assessment_tcs/datasets/DomainB_Enhanced/annotations.coco.json\"\n",
    "output_ann = \"/home/aamena/Assessment_tcs/datasets/DomainB_Enhanced/annotations_fixed.coco.json\"\n",
    "\n",
    "# Load annotation\n",
    "with open(ann_path, \"r\") as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "# Build map of filename â†’ size\n",
    "size_map = {}\n",
    "for fname in os.listdir(images_dir):\n",
    "    if not fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        continue\n",
    "    path = os.path.join(images_dir, fname)\n",
    "    w, h = Image.open(path).size\n",
    "    size_map[fname] = (w, h)\n",
    "\n",
    "updated = 0\n",
    "\n",
    "# Fix annotation sizes\n",
    "for img in coco[\"images\"]:\n",
    "    fname = img[\"file_name\"]\n",
    "    if fname in size_map:\n",
    "        w, h = size_map[fname]\n",
    "        img[\"width\"] = w\n",
    "        img[\"height\"] = h\n",
    "        updated += 1\n",
    "\n",
    "print(f\"Updated {updated} image entries.\")\n",
    "\n",
    "# Save new JSON\n",
    "with open(output_ann, \"w\") as f:\n",
    "    json.dump(coco, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", output_ann)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2153d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aamena/Assessment_tcs/.venv/lib/python3.10/site-packages/detectron2/model_zoo/model_zoo.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/19 17:41:51 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /home/aamena/Assessment_tcs/output/domain_a_baseline/model_final.pth ...\n",
      "\u001b[32m[11/19 17:41:51 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/19 17:41:51 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[11/19 17:41:51 d2.data.datasets.coco]: \u001b[0mLoaded 1728 images in COCO format from datasets/DomainB_Enhanced/annotations.coco.json\n",
      "\u001b[32m[11/19 17:41:51 d2.data.build]: \u001b[0mDistribution of instances among all 4 categories:\n",
      "\u001b[36m|   category    | #instances   |  category  | #instances   |  category  | #instances   |\n",
      "|:-------------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
      "| In-\"Pa\",-\"Sc\" | 0            |     In     | 1705         |     Pa     | 1240         |\n",
      "|      Sc       | 1162         |            |              |            |              |\n",
      "|     total     | 4107         |            |              |            |              |\u001b[0m\n",
      "\u001b[32m[11/19 17:41:51 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[11/19 17:41:51 d2.data.common]: \u001b[0mSerializing 1728 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/19 17:41:51 d2.data.common]: \u001b[0mSerialized dataset takes 1.70 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/19 17:41:51 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "ðŸš€ Starting evaluation...\n",
      "\u001b[32m[11/19 17:41:51 d2.evaluation.evaluator]: \u001b[0mStart inference on 1728 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aamena/Assessment_tcs/.venv/lib/python3.10/site-packages/torch/functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "W1119 17:41:51.953000 72591 torch/fx/_symbolic_trace.py:52] is_fx_tracing will return true for both fx.symbolic_trace and torch.export. Please use is_fx_tracing_symbolic_tracing() for specifically fx.symbolic_trace or torch.compiler.is_compiling() for specifically torch.export/compile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/19 17:41:52 d2.evaluation.evaluator]: \u001b[0mInference done 11/1728. Dataloading: 0.0007 s/iter. Inference: 0.0595 s/iter. Eval: 0.0013 s/iter. Total: 0.0616 s/iter. ETA=0:01:45\n",
      "\u001b[32m[11/19 17:41:57 d2.evaluation.evaluator]: \u001b[0mInference done 92/1728. Dataloading: 0.0008 s/iter. Inference: 0.0599 s/iter. Eval: 0.0013 s/iter. Total: 0.0621 s/iter. ETA=0:01:41\n",
      "\u001b[32m[11/19 17:42:02 d2.evaluation.evaluator]: \u001b[0mInference done 173/1728. Dataloading: 0.0008 s/iter. Inference: 0.0599 s/iter. Eval: 0.0012 s/iter. Total: 0.0620 s/iter. ETA=0:01:36\n",
      "\u001b[32m[11/19 17:42:07 d2.evaluation.evaluator]: \u001b[0mInference done 254/1728. Dataloading: 0.0009 s/iter. Inference: 0.0599 s/iter. Eval: 0.0012 s/iter. Total: 0.0620 s/iter. ETA=0:01:31\n",
      "\u001b[32m[11/19 17:42:12 d2.evaluation.evaluator]: \u001b[0mInference done 335/1728. Dataloading: 0.0009 s/iter. Inference: 0.0599 s/iter. Eval: 0.0013 s/iter. Total: 0.0621 s/iter. ETA=0:01:26\n",
      "\u001b[32m[11/19 17:42:17 d2.evaluation.evaluator]: \u001b[0mInference done 416/1728. Dataloading: 0.0009 s/iter. Inference: 0.0600 s/iter. Eval: 0.0012 s/iter. Total: 0.0621 s/iter. ETA=0:01:21\n",
      "\u001b[32m[11/19 17:42:22 d2.evaluation.evaluator]: \u001b[0mInference done 497/1728. Dataloading: 0.0009 s/iter. Inference: 0.0600 s/iter. Eval: 0.0012 s/iter. Total: 0.0621 s/iter. ETA=0:01:16\n",
      "\u001b[32m[11/19 17:42:27 d2.evaluation.evaluator]: \u001b[0mInference done 578/1728. Dataloading: 0.0009 s/iter. Inference: 0.0600 s/iter. Eval: 0.0012 s/iter. Total: 0.0622 s/iter. ETA=0:01:11\n",
      "\u001b[32m[11/19 17:42:32 d2.evaluation.evaluator]: \u001b[0mInference done 658/1728. Dataloading: 0.0009 s/iter. Inference: 0.0601 s/iter. Eval: 0.0012 s/iter. Total: 0.0622 s/iter. ETA=0:01:06\n",
      "\u001b[32m[11/19 17:42:37 d2.evaluation.evaluator]: \u001b[0mInference done 737/1728. Dataloading: 0.0009 s/iter. Inference: 0.0602 s/iter. Eval: 0.0013 s/iter. Total: 0.0624 s/iter. ETA=0:01:01\n",
      "\u001b[32m[11/19 17:42:43 d2.evaluation.evaluator]: \u001b[0mInference done 817/1728. Dataloading: 0.0008 s/iter. Inference: 0.0603 s/iter. Eval: 0.0013 s/iter. Total: 0.0624 s/iter. ETA=0:00:56\n",
      "\u001b[32m[11/19 17:42:48 d2.evaluation.evaluator]: \u001b[0mInference done 897/1728. Dataloading: 0.0009 s/iter. Inference: 0.0603 s/iter. Eval: 0.0013 s/iter. Total: 0.0624 s/iter. ETA=0:00:51\n",
      "\u001b[32m[11/19 17:42:53 d2.evaluation.evaluator]: \u001b[0mInference done 977/1728. Dataloading: 0.0009 s/iter. Inference: 0.0603 s/iter. Eval: 0.0013 s/iter. Total: 0.0624 s/iter. ETA=0:00:46\n",
      "\u001b[32m[11/19 17:42:58 d2.evaluation.evaluator]: \u001b[0mInference done 1057/1728. Dataloading: 0.0009 s/iter. Inference: 0.0603 s/iter. Eval: 0.0013 s/iter. Total: 0.0625 s/iter. ETA=0:00:41\n",
      "\u001b[32m[11/19 17:43:03 d2.evaluation.evaluator]: \u001b[0mInference done 1137/1728. Dataloading: 0.0009 s/iter. Inference: 0.0603 s/iter. Eval: 0.0013 s/iter. Total: 0.0625 s/iter. ETA=0:00:36\n",
      "\u001b[32m[11/19 17:43:08 d2.evaluation.evaluator]: \u001b[0mInference done 1217/1728. Dataloading: 0.0009 s/iter. Inference: 0.0603 s/iter. Eval: 0.0013 s/iter. Total: 0.0625 s/iter. ETA=0:00:31\n",
      "\u001b[32m[11/19 17:43:13 d2.evaluation.evaluator]: \u001b[0mInference done 1297/1728. Dataloading: 0.0009 s/iter. Inference: 0.0604 s/iter. Eval: 0.0013 s/iter. Total: 0.0625 s/iter. ETA=0:00:26\n",
      "\u001b[32m[11/19 17:43:18 d2.evaluation.evaluator]: \u001b[0mInference done 1377/1728. Dataloading: 0.0009 s/iter. Inference: 0.0604 s/iter. Eval: 0.0013 s/iter. Total: 0.0625 s/iter. ETA=0:00:21\n",
      "\u001b[32m[11/19 17:43:23 d2.evaluation.evaluator]: \u001b[0mInference done 1457/1728. Dataloading: 0.0009 s/iter. Inference: 0.0604 s/iter. Eval: 0.0013 s/iter. Total: 0.0626 s/iter. ETA=0:00:16\n",
      "\u001b[32m[11/19 17:43:28 d2.evaluation.evaluator]: \u001b[0mInference done 1537/1728. Dataloading: 0.0009 s/iter. Inference: 0.0604 s/iter. Eval: 0.0013 s/iter. Total: 0.0626 s/iter. ETA=0:00:11\n",
      "\u001b[32m[11/19 17:43:33 d2.evaluation.evaluator]: \u001b[0mInference done 1617/1728. Dataloading: 0.0009 s/iter. Inference: 0.0604 s/iter. Eval: 0.0013 s/iter. Total: 0.0626 s/iter. ETA=0:00:06\n",
      "\u001b[32m[11/19 17:43:38 d2.evaluation.evaluator]: \u001b[0mInference done 1697/1728. Dataloading: 0.0009 s/iter. Inference: 0.0604 s/iter. Eval: 0.0013 s/iter. Total: 0.0626 s/iter. ETA=0:00:01\n",
      "\u001b[32m[11/19 17:43:40 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:47.864305 (0.062603 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/19 17:43:40 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:44 (0.060404 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/19 17:43:40 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[11/19 17:43:40 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to /home/aamena/Assessment_tcs/output/domain_a_baseline/validation_results/coco_instances_results.json\n",
      "\u001b[32m[11/19 17:43:40 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[11/19 17:43:40 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[11/19 17:43:40 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.07 seconds.\n",
      "\u001b[32m[11/19 17:43:40 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[11/19 17:43:40 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.447\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.757\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.453\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.059\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.301\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.522\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.260\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.534\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.534\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.199\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.423\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.606\n",
      "\u001b[32m[11/19 17:43:40 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 44.689 | 75.689 | 45.271 | 5.866 | 30.119 | 52.208 |\n",
      "\u001b[32m[11/19 17:43:40 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category      | AP     | category   | AP     | category   | AP     |\n",
      "|:--------------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| In-\"Pa\",-\"Sc\" | nan    | In         | 28.807 | Pa         | 64.237 |\n",
      "| Sc            | 41.024 |            |        |            |        |\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[11/19 17:43:40 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
      "\u001b[32m[11/19 17:43:40 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.25 seconds.\n",
      "\u001b[32m[11/19 17:43:40 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[11/19 17:43:40 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.02 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.361\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.703\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.335\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.209\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.429\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.220\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.446\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.447\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.163\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.347\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499\n",
      "\u001b[32m[11/19 17:43:40 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 36.075 | 70.262 | 33.511 | 2.668 | 20.879 | 42.925 |\n",
      "\u001b[32m[11/19 17:43:40 d2.evaluation.coco_evaluation]: \u001b[0mPer-category segm AP: \n",
      "| category      | AP     | category   | AP     | category   | AP     |\n",
      "|:--------------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| In-\"Pa\",-\"Sc\" | nan    | In         | 23.417 | Pa         | 59.165 |\n",
      "| Sc            | 25.643 |            |        |            |        |\n",
      "\n",
      "================================\n",
      " FINAL EVALUATION RESULTS\n",
      "================================\n",
      "OrderedDict([('bbox', {'AP': 44.689166150641604, 'AP50': 75.6889879618317, 'AP75': 45.27105027228412, 'APs': 5.8663694145968925, 'APm': 30.118983353537825, 'APl': 52.20752527416727, 'AP-In-\"Pa\",-\"Sc\"': nan, 'AP-In': 28.806505628242295, 'AP-Pa': 64.2371884049813, 'AP-Sc': 41.023804418701246}), ('segm', {'AP': 36.07489153353454, 'AP50': 70.26205712765055, 'AP75': 33.510787276726944, 'APs': 2.667839908564984, 'APm': 20.879161991158984, 'APl': 42.92513304410435, 'AP-In-\"Pa\",-\"Sc\"': nan, 'AP-In': 23.41678847895717, 'AP-Pa': 59.165024721138415, 'AP-Sc': 25.64286140050803})])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import detectron2\n",
    "\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data import DatasetMapper\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1. Register Dataset\n",
    "# ----------------------------------------------------------\n",
    "VAL_DATASET_NAME = \"neu_valid\"\n",
    "VAL_JSON = \"datasets/DomainB_Enhanced/annotations.coco.json\"\n",
    "VAL_IMG_DIR = \"datasets/DomainB_Enhanced/images\"\n",
    "\n",
    "# Register once\n",
    "try:\n",
    "    register_coco_instances(VAL_DATASET_NAME, {}, VAL_JSON, VAL_IMG_DIR)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2. Configuration\n",
    "# ----------------------------------------------------------\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\n",
    "    \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    "))\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "\n",
    "cfg.OUTPUT_DIR = \"/home/aamena/Assessment_tcs/output/domain_a_baseline\"\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3. Evaluator + Loader\n",
    "# ----------------------------------------------------------\n",
    "predictor = DefaultPredictor(cfg)\n",
    "test_mapper = DatasetMapper(cfg, is_train=False)\n",
    "\n",
    "# FIXED: correct arguments\n",
    "eval_dataloader = build_detection_test_loader(\n",
    "    cfg,\n",
    "    VAL_DATASET_NAME,\n",
    "    mapper=test_mapper\n",
    ")\n",
    "\n",
    "evaluator = COCOEvaluator(\n",
    "    VAL_DATASET_NAME,\n",
    "    cfg,\n",
    "    False,\n",
    "    output_dir=os.path.join(cfg.OUTPUT_DIR, \"validation_results\")\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4. Run Evaluation\n",
    "# ----------------------------------------------------------\n",
    "print(\"ðŸš€ Starting evaluation...\")\n",
    "\n",
    "results = inference_on_dataset(\n",
    "    predictor.model,\n",
    "    eval_dataloader,\n",
    "    evaluator\n",
    ")\n",
    "\n",
    "print(\"\\n================================\")\n",
    "print(\" FINAL EVALUATION RESULTS\")\n",
    "print(\"================================\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11c9426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Bounding Box Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AP</th>\n",
       "      <td>44.689166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP50</th>\n",
       "      <td>75.688988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP75</th>\n",
       "      <td>45.271050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APs</th>\n",
       "      <td>5.866369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APm</th>\n",
       "      <td>30.118983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APl</th>\n",
       "      <td>52.207525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-In-\"Pa\",-\"Sc\"</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-In</th>\n",
       "      <td>28.806506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-Pa</th>\n",
       "      <td>64.237188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-Sc</th>\n",
       "      <td>41.023804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Score\n",
       "AP                44.689166\n",
       "AP50              75.688988\n",
       "AP75              45.271050\n",
       "APs                5.866369\n",
       "APm               30.118983\n",
       "APl               52.207525\n",
       "AP-In-\"Pa\",-\"Sc\"        NaN\n",
       "AP-In             28.806506\n",
       "AP-Pa             64.237188\n",
       "AP-Sc             41.023804"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Œ Segmentation Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AP</th>\n",
       "      <td>36.074892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP50</th>\n",
       "      <td>70.262057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP75</th>\n",
       "      <td>33.510787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APs</th>\n",
       "      <td>2.667840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APm</th>\n",
       "      <td>20.879162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APl</th>\n",
       "      <td>42.925133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-In-\"Pa\",-\"Sc\"</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-In</th>\n",
       "      <td>23.416788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-Pa</th>\n",
       "      <td>59.165025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-Sc</th>\n",
       "      <td>25.642861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Score\n",
       "AP                36.074892\n",
       "AP50              70.262057\n",
       "AP75              33.510787\n",
       "APs                2.667840\n",
       "APm               20.879162\n",
       "APl               42.925133\n",
       "AP-In-\"Pa\",-\"Sc\"        NaN\n",
       "AP-In             23.416788\n",
       "AP-Pa             59.165025\n",
       "AP-Sc             25.642861"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------\n",
    "# Convert Detectron2 COCO results â†’ DataFrames\n",
    "# ------------------------------------------\n",
    "bbox_results = results[\"bbox\"]\n",
    "segm_results = results[\"segm\"]\n",
    "\n",
    "bbox_df = pd.DataFrame(bbox_results, index=[0]).T.rename(columns={0: \"Score\"})\n",
    "segm_df = pd.DataFrame(segm_results, index=[0]).T.rename(columns={0: \"Score\"})\n",
    "\n",
    "print(\"ðŸ“Œ Bounding Box Metrics\")\n",
    "display(bbox_df)\n",
    "\n",
    "print(\"\\nðŸ“Œ Segmentation Metrics\")\n",
    "display(segm_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
